{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humor SFT Model Evaluation\n",
    "\n",
    "Compare base model vs fine-tuned model on eval set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "print(\"‚úì Base model loaded\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"\\nLoading fine-tuned model...\")\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2b-it\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    ),\n",
    "    \"./humor_sft_output\"\n",
    ")\n",
    "print(\"‚úì Fine-tuned model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "with open('data/humor_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Same split as training (80/20, seed 42)\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "eval_examples = split_dataset[\"test\"]\n",
    "print(f\"Loaded {len(eval_examples)} eval examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, messages, max_new_tokens=100):\n",
    "    \"\"\"Generate response from model\"\"\"\n",
    "    # Format conversation (all messages except the last assistant response)\n",
    "    prompt_messages = []\n",
    "    for msg in messages:\n",
    "        prompt_messages.append(msg)\n",
    "        if msg[\"role\"] == \"assistant\":\n",
    "            # Stop before the golden response\n",
    "            if msg == messages[-1]:  # If this is the last message\n",
    "                prompt_messages = messages[:-1]  # Remove it\n",
    "                break\n",
    "    \n",
    "    # Apply chat template with generation prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        prompt_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, example in enumerate(eval_examples):\n",
    "    messages = example[\"messages\"]\n",
    "    sample_id = example.get(\"id\", f\"eval_{i}\")\n",
    "    \n",
    "    # Get the conversation context (all but last message)\n",
    "    context = messages[:-1]\n",
    "    \n",
    "    # Get the golden (funny) response\n",
    "    golden_response = messages[-1][\"content\"]\n",
    "    \n",
    "    # Generate from base model\n",
    "    print(f\"Generating {i+1}/{len(eval_examples)}...\")\n",
    "    base_response = generate_response(base_model, messages)\n",
    "    \n",
    "    # Generate from fine-tuned model\n",
    "    finetuned_response = generate_response(finetuned_model, messages)\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": sample_id,\n",
    "        \"context\": \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in context]),\n",
    "        \"golden\": golden_response,\n",
    "        \"base_model\": base_response,\n",
    "        \"finetuned_model\": finetuned_response,\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úì Generated responses for {len(results)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Display function for pretty HTML\n",
    "def display_comparison(idx):\n",
    "    row = df.iloc[idx]\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"border: 2px solid #ccc; padding: 15px; margin: 10px 0; border-radius: 5px;\">\n",
    "        <h3 style=\"color: #2c3e50;\">Example {idx+1}/{len(df)} - ID: {row['id']}</h3>\n",
    "        \n",
    "        <div style=\"background: #f8f9fa; padding: 10px; margin: 10px 0; border-radius: 3px;\">\n",
    "            <h4 style=\"color: #34495e;\">üìù Context:</h4>\n",
    "            <pre style=\"white-space: pre-wrap; font-family: monospace;\">{row['context']}</pre>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: #d4edda; padding: 10px; margin: 10px 0; border-radius: 3px; border-left: 4px solid #28a745;\">\n",
    "            <h4 style=\"color: #155724;\">üéØ Golden Response (Training Target):</h4>\n",
    "            <p style=\"margin: 5px 0; font-family: monospace;\">{row['golden']}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: #d1ecf1; padding: 10px; margin: 10px 0; border-radius: 3px; border-left: 4px solid #17a2b8;\">\n",
    "            <h4 style=\"color: #0c5460;\">ü§ñ Base Model:</h4>\n",
    "            <p style=\"margin: 5px 0; font-family: monospace;\">{row['base_model']}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: #fff3cd; padding: 10px; margin: 10px 0; border-radius: 3px; border-left: 4px solid #ffc107;\">\n",
    "            <h4 style=\"color: #856404;\">üî• Fine-tuned Model:</h4>\n",
    "            <p style=\"margin: 5px 0; font-family: monospace;\">{row['finetuned_model']}</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "# Display all examples\n",
    "for i in range(len(df)):\n",
    "    display_comparison(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple heuristics to check if responses are different\n",
    "base_lengths = df['base_model'].str.len()\n",
    "ft_lengths = df['finetuned_model'].str.len()\n",
    "golden_lengths = df['golden'].str.len()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResponse length (characters):\")\n",
    "print(f\"  Golden:     {golden_lengths.mean():.0f} ¬± {golden_lengths.std():.0f}\")\n",
    "print(f\"  Base:       {base_lengths.mean():.0f} ¬± {base_lengths.std():.0f}\")\n",
    "print(f\"  Fine-tuned: {ft_lengths.mean():.0f} ¬± {ft_lengths.std():.0f}\")\n",
    "\n",
    "# Check how many are identical\n",
    "identical = (df['base_model'] == df['finetuned_model']).sum()\n",
    "print(f\"\\nIdentical responses: {identical}/{len(df)} ({100*identical/len(df):.1f}%)\")\n",
    "\n",
    "# Check for humor indicators (very rough)\n",
    "humor_words = ['!', '?', 'haha', 'lol', 'joke', 'pun', 'funny', 'actually']\n",
    "base_humor = df['base_model'].str.lower().str.contains('|'.join(humor_words)).sum()\n",
    "ft_humor = df['finetuned_model'].str.lower().str.contains('|'.join(humor_words)).sum()\n",
    "golden_humor = df['golden'].str.lower().str.contains('|'.join(humor_words)).sum()\n",
    "\n",
    "print(f\"\\nContains humor indicators:\")\n",
    "print(f\"  Golden:     {golden_humor}/{len(df)} ({100*golden_humor/len(df):.1f}%)\")\n",
    "print(f\"  Base:       {base_humor}/{len(df)} ({100*base_humor/len(df):.1f}%)\")\n",
    "print(f\"  Fine-tuned: {ft_humor}/{len(df)} ({100*ft_humor/len(df):.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for analysis\n",
    "df.to_csv(\"eval_results.csv\", index=False)\n",
    "print(\"‚úì Saved results to eval_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
