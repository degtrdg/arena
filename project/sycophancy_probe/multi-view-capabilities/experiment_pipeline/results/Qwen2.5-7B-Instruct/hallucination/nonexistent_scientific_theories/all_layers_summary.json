{
  "layer_4": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 0.987500011920929,
      "invented_recipes": 0.8500000238418579,
      "fabricating_legal_precedents": 0.6000000238418579,
      "nonsense_medical_advice": 0.8374999761581421,
      "imaginary_geography": 0.925000011920929,
      "general": 0.824999988079071
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 0.9876543209876543,
      "invented_recipes": 0.8235294117647058,
      "fabricating_legal_precedents": 0.3333333333333333,
      "nonsense_medical_advice": 0.8059701492537313,
      "imaginary_geography": 0.918918918918919,
      "general": 0.7878787878787878
    }
  },
  "layer_8": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9750000238418579,
      "fabricating_legal_precedents": 0.762499988079071,
      "nonsense_medical_advice": 0.9750000238418579,
      "imaginary_geography": 0.987500011920929,
      "general": 0.9624999761581421
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9743589743589743,
      "fabricating_legal_precedents": 0.6885245901639344,
      "nonsense_medical_advice": 0.9743589743589743,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.961038961038961
    }
  },
  "layer_12": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9125000238418579,
      "fabricating_legal_precedents": 0.6875,
      "nonsense_medical_advice": 0.9125000238418579,
      "imaginary_geography": 0.987500011920929,
      "general": 0.9125000238418579
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9041095890410958,
      "fabricating_legal_precedents": 0.5454545454545454,
      "nonsense_medical_advice": 0.9041095890410958,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.9041095890410958
    }
  },
  "layer_16": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9624999761581421,
      "fabricating_legal_precedents": 0.8999999761581421,
      "nonsense_medical_advice": 0.9624999761581421,
      "imaginary_geography": 1.0,
      "general": 0.925000011920929
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.961038961038961,
      "fabricating_legal_precedents": 0.8888888888888888,
      "nonsense_medical_advice": 0.961038961038961,
      "imaginary_geography": 1.0,
      "general": 0.918918918918919
    }
  },
  "layer_20": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.987500011920929,
      "fabricating_legal_precedents": 0.9750000238418579,
      "nonsense_medical_advice": 0.9624999761581421,
      "imaginary_geography": 1.0,
      "general": 0.987500011920929
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.9873417721518988,
      "fabricating_legal_precedents": 0.975,
      "nonsense_medical_advice": 0.961038961038961,
      "imaginary_geography": 1.0,
      "general": 0.9873417721518988
    }
  },
  "layer_24": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.875,
      "fabricating_legal_precedents": 0.737500011920929,
      "nonsense_medical_advice": 0.8500000238418579,
      "imaginary_geography": 0.824999988079071,
      "general": 0.8500000238418579
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.8571428571428571,
      "fabricating_legal_precedents": 0.6440677966101694,
      "nonsense_medical_advice": 0.8235294117647058,
      "imaginary_geography": 0.7878787878787878,
      "general": 0.8235294117647058
    }
  },
  "layer_28": {
    "accuracy": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.862500011920929,
      "fabricating_legal_precedents": 0.5249999761581421,
      "nonsense_medical_advice": 0.8374999761581421,
      "imaginary_geography": 0.8500000238418579,
      "general": 0.800000011920929
    },
    "f1_scores": {
      "nonexistent_scientific_theories (in-context)": 1.0,
      "invented_recipes": 0.8405797101449275,
      "fabricating_legal_precedents": 0.09523809523809523,
      "nonsense_medical_advice": 0.8059701492537313,
      "imaginary_geography": 0.8235294117647058,
      "general": 0.75
    }
  }
}