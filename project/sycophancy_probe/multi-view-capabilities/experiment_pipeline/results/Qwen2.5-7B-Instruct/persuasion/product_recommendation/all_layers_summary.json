{
  "layer_4": {
    "accuracy": {
      "product_recommendation (in-context)": 0.8500000238418579,
      "justifying_discrimination": 0.512499988079071,
      "general": 0.800000011920929,
      "recipe_recommendation_persuasion": 0.8500000238418579,
      "conspiracy_theory_endorsement": 0.762499988079071,
      "book_recommendation_defense": 0.7749999761581421
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.8461538461538461,
      "justifying_discrimination": 0.09302325581395349,
      "general": 0.7714285714285715,
      "recipe_recommendation_persuasion": 0.8666666666666667,
      "conspiracy_theory_endorsement": 0.7397260273972602,
      "book_recommendation_defense": 0.8085106382978723
    }
  },
  "layer_8": {
    "accuracy": {
      "product_recommendation (in-context)": 0.925000011920929,
      "justifying_discrimination": 0.512499988079071,
      "general": 0.7875000238418579,
      "recipe_recommendation_persuasion": 0.8374999761581421,
      "conspiracy_theory_endorsement": 0.7124999761581421,
      "book_recommendation_defense": 0.824999988079071
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.9210526315789473,
      "justifying_discrimination": 0.09302325581395349,
      "general": 0.7733333333333333,
      "recipe_recommendation_persuasion": 0.8571428571428571,
      "conspiracy_theory_endorsement": 0.735632183908046,
      "book_recommendation_defense": 0.8478260869565217
    }
  },
  "layer_12": {
    "accuracy": {
      "product_recommendation (in-context)": 0.949999988079071,
      "justifying_discrimination": 0.512499988079071,
      "general": 0.800000011920929,
      "recipe_recommendation_persuasion": 0.887499988079071,
      "conspiracy_theory_endorsement": 0.699999988079071,
      "book_recommendation_defense": 0.949999988079071
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.9473684210526315,
      "justifying_discrimination": 0.09302325581395349,
      "general": 0.7714285714285715,
      "recipe_recommendation_persuasion": 0.896551724137931,
      "conspiracy_theory_endorsement": 0.7,
      "book_recommendation_defense": 0.9512195121951219
    }
  },
  "layer_16": {
    "accuracy": {
      "product_recommendation (in-context)": 0.9625000357627869,
      "justifying_discrimination": 0.512499988079071,
      "general": 0.800000011920929,
      "recipe_recommendation_persuasion": 0.887499988079071,
      "conspiracy_theory_endorsement": 0.6625000238418579,
      "book_recommendation_defense": 0.925000011920929
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.961038961038961,
      "justifying_discrimination": 0.23529411764705882,
      "general": 0.7837837837837838,
      "recipe_recommendation_persuasion": 0.898876404494382,
      "conspiracy_theory_endorsement": 0.6966292134831461,
      "book_recommendation_defense": 0.9285714285714286
    }
  },
  "layer_20": {
    "accuracy": {
      "product_recommendation (in-context)": 0.9750000238418579,
      "justifying_discrimination": 0.512499988079071,
      "general": 0.800000011920929,
      "recipe_recommendation_persuasion": 0.862500011920929,
      "conspiracy_theory_endorsement": 0.75,
      "book_recommendation_defense": 0.9375
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.9743589743589743,
      "justifying_discrimination": 0.04878048780487805,
      "general": 0.7647058823529411,
      "recipe_recommendation_persuasion": 0.8791208791208791,
      "conspiracy_theory_endorsement": 0.7368421052631579,
      "book_recommendation_defense": 0.9397590361445783
    }
  },
  "layer_24": {
    "accuracy": {
      "product_recommendation (in-context)": 0.9625000357627869,
      "justifying_discrimination": 0.48750001192092896,
      "general": 0.800000011920929,
      "recipe_recommendation_persuasion": 0.887499988079071,
      "conspiracy_theory_endorsement": 0.7250000238418579,
      "book_recommendation_defense": 0.8999999761581421
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.961038961038961,
      "justifying_discrimination": 0.0,
      "general": 0.7575757575757576,
      "recipe_recommendation_persuasion": 0.896551724137931,
      "conspiracy_theory_endorsement": 0.6944444444444444,
      "book_recommendation_defense": 0.9024390243902439
    }
  },
  "layer_28": {
    "accuracy": {
      "product_recommendation (in-context)": 0.925000011920929,
      "justifying_discrimination": 0.5,
      "general": 0.8125,
      "recipe_recommendation_persuasion": 0.8374999761581421,
      "conspiracy_theory_endorsement": 0.737500011920929,
      "book_recommendation_defense": 0.8500000238418579
    },
    "f1_scores": {
      "product_recommendation (in-context)": 0.9210526315789473,
      "justifying_discrimination": 0.0,
      "general": 0.7761194029850746,
      "recipe_recommendation_persuasion": 0.8539325842696629,
      "conspiracy_theory_endorsement": 0.6956521739130435,
      "book_recommendation_defense": 0.8571428571428571
    }
  }
}