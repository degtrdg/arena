{
  "layer_4": {
    "accuracy": {
      "personal_preference (in-context)": 0.9000000357627869,
      "general": 0.8500000238418579,
      "promoting_self-harm": 0.862500011920929,
      "illegal_activities": 0.9375,
      "generating_malicious_code": 0.824999988079071,
      "recipe_preference": 0.875
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.8974358974358975,
      "general": 0.8536585365853658,
      "promoting_self-harm": 0.8674698795180723,
      "illegal_activities": 0.9397590361445783,
      "generating_malicious_code": 0.8333333333333334,
      "recipe_preference": 0.8611111111111112
    }
  },
  "layer_8": {
    "accuracy": {
      "personal_preference (in-context)": 0.9000000357627869,
      "general": 0.875,
      "promoting_self-harm": 0.8500000238418579,
      "illegal_activities": 0.925000011920929,
      "generating_malicious_code": 0.824999988079071,
      "recipe_preference": 0.887499988079071
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.8974358974358975,
      "general": 0.8780487804878049,
      "promoting_self-harm": 0.8536585365853658,
      "illegal_activities": 0.926829268292683,
      "generating_malicious_code": 0.8333333333333334,
      "recipe_preference": 0.8831168831168831
    }
  },
  "layer_12": {
    "accuracy": {
      "personal_preference (in-context)": 0.9000000357627869,
      "general": 0.887499988079071,
      "promoting_self-harm": 0.862500011920929,
      "illegal_activities": 0.9375,
      "generating_malicious_code": 0.8125,
      "recipe_preference": 0.862500011920929
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.8974358974358975,
      "general": 0.891566265060241,
      "promoting_self-harm": 0.8674698795180723,
      "illegal_activities": 0.9397590361445783,
      "generating_malicious_code": 0.8192771084337349,
      "recipe_preference": 0.8607594936708861
    }
  },
  "layer_16": {
    "accuracy": {
      "personal_preference (in-context)": 0.9125000238418579,
      "general": 0.8374999761581421,
      "promoting_self-harm": 0.824999988079071,
      "illegal_activities": 0.9125000238418579,
      "generating_malicious_code": 0.824999988079071,
      "recipe_preference": 0.862500011920929
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.9113924050632911,
      "general": 0.8470588235294118,
      "promoting_self-harm": 0.8372093023255814,
      "illegal_activities": 0.9176470588235294,
      "generating_malicious_code": 0.8372093023255814,
      "recipe_preference": 0.8607594936708861
    }
  },
  "layer_20": {
    "accuracy": {
      "personal_preference (in-context)": 0.9125000238418579,
      "general": 0.8374999761581421,
      "promoting_self-harm": 0.800000011920929,
      "illegal_activities": 0.887499988079071,
      "generating_malicious_code": 0.8500000238418579,
      "recipe_preference": 0.875
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.9113924050632911,
      "general": 0.8505747126436781,
      "promoting_self-harm": 0.8222222222222222,
      "illegal_activities": 0.896551724137931,
      "generating_malicious_code": 0.8604651162790697,
      "recipe_preference": 0.875
    }
  },
  "layer_24": {
    "accuracy": {
      "personal_preference (in-context)": 0.875,
      "general": 0.8500000238418579,
      "promoting_self-harm": 0.862500011920929,
      "illegal_activities": 0.925000011920929,
      "generating_malicious_code": 0.8125,
      "recipe_preference": 0.8374999761581421
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.868421052631579,
      "general": 0.8536585365853658,
      "promoting_self-harm": 0.8674698795180723,
      "illegal_activities": 0.926829268292683,
      "generating_malicious_code": 0.8235294117647058,
      "recipe_preference": 0.8311688311688312
    }
  },
  "layer_28": {
    "accuracy": {
      "personal_preference (in-context)": 0.8375000357627869,
      "general": 0.824999988079071,
      "promoting_self-harm": 0.7749999761581421,
      "illegal_activities": 0.8999999761581421,
      "generating_malicious_code": 0.8374999761581421,
      "recipe_preference": 0.8374999761581421
    },
    "f1_scores": {
      "personal_preference (in-context)": 0.8311688311688312,
      "general": 0.8409090909090909,
      "promoting_self-harm": 0.8163265306122449,
      "illegal_activities": 0.9069767441860465,
      "generating_malicious_code": 0.8505747126436781,
      "recipe_preference": 0.8311688311688312
    }
  }
}