{
  "layer_5": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.9750000238418579,
      "invented_recipes": 0.949999988079071,
      "general": 0.862500011920929,
      "nonsense_medical_advice": 0.762499988079071,
      "fabricating_legal_precedents": 0.800000011920929,
      "nonexistent_scientific_theories": 0.7749999761581421
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.975,
      "invented_recipes": 0.9473684210526315,
      "general": 0.8705882352941177,
      "nonsense_medical_advice": 0.8041237113402062,
      "fabricating_legal_precedents": 0.8333333333333334,
      "nonexistent_scientific_theories": 0.8085106382978723
    }
  },
  "layer_10": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.9625000357627869,
      "invented_recipes": 0.9624999761581421,
      "general": 0.875,
      "nonsense_medical_advice": 0.762499988079071,
      "fabricating_legal_precedents": 0.824999988079071,
      "nonexistent_scientific_theories": 0.7875000238418579
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.9629629629629629,
      "invented_recipes": 0.961038961038961,
      "general": 0.8809523809523809,
      "nonsense_medical_advice": 0.8041237113402062,
      "fabricating_legal_precedents": 0.851063829787234,
      "nonexistent_scientific_theories": 0.8210526315789474
    }
  },
  "layer_15": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.9625000357627869,
      "invented_recipes": 0.9624999761581421,
      "general": 0.8500000238418579,
      "nonsense_medical_advice": 0.675000011920929,
      "fabricating_legal_precedents": 0.7250000238418579,
      "nonexistent_scientific_theories": 0.7749999761581421
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.963855421686747,
      "invented_recipes": 0.963855421686747,
      "general": 0.8695652173913043,
      "nonsense_medical_advice": 0.7547169811320755,
      "fabricating_legal_precedents": 0.7843137254901961,
      "nonexistent_scientific_theories": 0.8163265306122449
    }
  },
  "layer_20": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.9625000357627869,
      "invented_recipes": 0.9624999761581421,
      "general": 0.8500000238418579,
      "nonsense_medical_advice": 0.6875,
      "fabricating_legal_precedents": 0.737500011920929,
      "nonexistent_scientific_theories": 0.7749999761581421
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.963855421686747,
      "invented_recipes": 0.963855421686747,
      "general": 0.8695652173913043,
      "nonsense_medical_advice": 0.7619047619047619,
      "fabricating_legal_precedents": 0.7920792079207921,
      "nonexistent_scientific_theories": 0.8163265306122449
    }
  },
  "layer_25": {
    "accuracy": {
      "imaginary_geography (in-context)": 1.0,
      "invented_recipes": 1.0,
      "general": 0.9624999761581421,
      "nonsense_medical_advice": 0.9750000238418579,
      "fabricating_legal_precedents": 0.987500011920929,
      "nonexistent_scientific_theories": 0.9375
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 1.0,
      "invented_recipes": 1.0,
      "general": 0.9629629629629629,
      "nonsense_medical_advice": 0.975,
      "fabricating_legal_precedents": 0.9876543209876543,
      "nonexistent_scientific_theories": 0.9382716049382716
    }
  },
  "layer_30": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.9625000357627869,
      "invented_recipes": 1.0,
      "general": 0.9125000238418579,
      "nonsense_medical_advice": 0.762499988079071,
      "fabricating_legal_precedents": 0.8374999761581421,
      "nonexistent_scientific_theories": 0.800000011920929
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.963855421686747,
      "invented_recipes": 1.0,
      "general": 0.9195402298850575,
      "nonsense_medical_advice": 0.8080808080808081,
      "fabricating_legal_precedents": 0.8602150537634409,
      "nonexistent_scientific_theories": 0.8333333333333334
    }
  },
  "layer_35": {
    "accuracy": {
      "imaginary_geography (in-context)": 1.0,
      "invented_recipes": 1.0,
      "general": 0.949999988079071,
      "nonsense_medical_advice": 0.925000011920929,
      "fabricating_legal_precedents": 0.949999988079071,
      "nonexistent_scientific_theories": 0.949999988079071
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 1.0,
      "invented_recipes": 1.0,
      "general": 0.9512195121951219,
      "nonsense_medical_advice": 0.9302325581395349,
      "fabricating_legal_precedents": 0.9523809523809523,
      "nonexistent_scientific_theories": 0.95
    }
  },
  "layer_39": {
    "accuracy": {
      "imaginary_geography (in-context)": 0.987500011920929,
      "invented_recipes": 1.0,
      "general": 0.9624999761581421,
      "nonsense_medical_advice": 0.949999988079071,
      "fabricating_legal_precedents": 0.949999988079071,
      "nonexistent_scientific_theories": 0.987500011920929
    },
    "f1_scores": {
      "imaginary_geography (in-context)": 0.9873417721518988,
      "invented_recipes": 1.0,
      "general": 0.9629629629629629,
      "nonsense_medical_advice": 0.9523809523809523,
      "fabricating_legal_precedents": 0.95,
      "nonexistent_scientific_theories": 0.9876543209876543
    }
  }
}