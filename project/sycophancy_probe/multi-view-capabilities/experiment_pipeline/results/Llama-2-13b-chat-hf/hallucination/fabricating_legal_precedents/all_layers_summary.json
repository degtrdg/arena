{
  "layer_5": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.824999988079071,
      "invented_recipes": 0.925000011920929,
      "imaginary_geography": 0.9750000238418579,
      "general": 0.862500011920929,
      "nonsense_medical_advice": 0.8125,
      "nonexistent_scientific_theories": 0.800000011920929
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.8478260869565217,
      "invented_recipes": 0.918918918918919,
      "imaginary_geography": 0.975,
      "general": 0.8674698795180723,
      "nonsense_medical_advice": 0.8351648351648352,
      "nonexistent_scientific_theories": 0.8222222222222222
    }
  },
  "layer_10": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.8375000357627869,
      "invented_recipes": 0.925000011920929,
      "imaginary_geography": 0.987500011920929,
      "general": 0.875,
      "nonsense_medical_advice": 0.824999988079071,
      "nonexistent_scientific_theories": 0.8125
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.8602150537634409,
      "invented_recipes": 0.918918918918919,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.8780487804878049,
      "nonsense_medical_advice": 0.8444444444444444,
      "nonexistent_scientific_theories": 0.8314606741573034
    }
  },
  "layer_15": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.8500000238418579,
      "invented_recipes": 0.9125000238418579,
      "imaginary_geography": 0.987500011920929,
      "general": 0.875,
      "nonsense_medical_advice": 0.8374999761581421,
      "nonexistent_scientific_theories": 0.8374999761581421
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.8666666666666667,
      "invented_recipes": 0.9041095890410958,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.8780487804878049,
      "nonsense_medical_advice": 0.8470588235294118,
      "nonexistent_scientific_theories": 0.8505747126436781
    }
  },
  "layer_20": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.987500011920929,
      "invented_recipes": 0.9750000238418579,
      "imaginary_geography": 0.9750000238418579,
      "general": 0.9750000238418579,
      "nonsense_medical_advice": 0.9750000238418579,
      "nonexistent_scientific_theories": 1.0
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.9876543209876543,
      "invented_recipes": 0.9743589743589743,
      "imaginary_geography": 0.9743589743589743,
      "general": 0.975,
      "nonsense_medical_advice": 0.975,
      "nonexistent_scientific_theories": 1.0
    }
  },
  "layer_25": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.987500011920929,
      "invented_recipes": 0.9750000238418579,
      "imaginary_geography": 0.987500011920929,
      "general": 0.9750000238418579,
      "nonsense_medical_advice": 0.987500011920929,
      "nonexistent_scientific_theories": 0.987500011920929
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.9876543209876543,
      "invented_recipes": 0.9743589743589743,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.9743589743589743,
      "nonsense_medical_advice": 0.9873417721518988,
      "nonexistent_scientific_theories": 0.9873417721518988
    }
  },
  "layer_30": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.987500011920929,
      "invented_recipes": 0.987500011920929,
      "imaginary_geography": 0.987500011920929,
      "general": 0.9750000238418579,
      "nonsense_medical_advice": 0.9750000238418579,
      "nonexistent_scientific_theories": 0.987500011920929
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.9876543209876543,
      "invented_recipes": 0.9873417721518988,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.975,
      "nonsense_medical_advice": 0.975,
      "nonexistent_scientific_theories": 0.9876543209876543
    }
  },
  "layer_35": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.987500011920929,
      "invented_recipes": 0.987500011920929,
      "imaginary_geography": 0.987500011920929,
      "general": 0.9375,
      "nonsense_medical_advice": 0.9624999761581421,
      "nonexistent_scientific_theories": 0.949999988079071
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.9876543209876543,
      "invented_recipes": 0.9873417721518988,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.9382716049382716,
      "nonsense_medical_advice": 0.9629629629629629,
      "nonexistent_scientific_theories": 0.9512195121951219
    }
  },
  "layer_39": {
    "accuracy": {
      "fabricating_legal_precedents (in-context)": 0.9750000238418579,
      "invented_recipes": 0.949999988079071,
      "imaginary_geography": 0.987500011920929,
      "general": 0.887499988079071,
      "nonsense_medical_advice": 0.9375,
      "nonexistent_scientific_theories": 0.8500000238418579
    },
    "f1_scores": {
      "fabricating_legal_precedents (in-context)": 0.975609756097561,
      "invented_recipes": 0.9473684210526315,
      "imaginary_geography": 0.9873417721518988,
      "general": 0.891566265060241,
      "nonsense_medical_advice": 0.9397590361445783,
      "nonexistent_scientific_theories": 0.8636363636363636
    }
  }
}